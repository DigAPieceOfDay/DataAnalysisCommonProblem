# DataAnalysisCommonProblem
常见数据分析面试题


1.简要介绍下svm  
SVM，全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开
原理推导见：http://blog.csdn.net/v_july_v/article/details/7624837

2.简要介绍下tensorflow的计算图  
Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个节点都是计算图上的一个Tensor, 也就是张量，而节点之间的边描述了计算之间的依赖关系(定义时)和数学操作(运算时)

3.一般，K-NN 最近邻方法在什么情况下效果较好  
knn针对，整体样本应该具有典型性好，样本较少，比较适宜。便于发挥出其求近邻的优势，若样本呈现团状分布就无法计算近邻。  

4.有关分类算法的准确率，召回率，F1值  
对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不
正确，4 种情况出现的总数分别记作：  
  TP——将正类预测为正类数  
  FN——将正类预测为负类数  
  FP——将负类预测为正类数  
  TN——将负类预测为负类数  
由此：  
  精准率定义为：P = TP / (TP + FP)  
  召回率定义为：R = TP / (TP + FN)  
  F1 值定义为： F1 = 2 P R / (P + R)  
  
5.缺失值处理方法   
数据清理中，处理缺失值的方法有两种：   
删除法:  
 1）删除观察样本  
 2）删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除  
 3）使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析  
 4）改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差  
查补法：均值插补、回归插补、抽样填补等  
成对删除与改变权重为一类  
估算与查补法为一类  

6.在K-Means中如何拾取k  
K-Means 算法的最大缺点是不能自动选择分类数k，常见的确定k的方法有:  
- 根据先验知识来确定
- k=sqrt(N/2) N为样本数  
- 拐点法：把聚类结果的F-test值对聚类个数的曲线画出来，选择图中的拐点  
- 基于信息准则判断，如果模型有似然函数，则可以用 BIC、 DIC 来进行决策  
具体的 k 的选择往往和业务联系紧密，如希望能将用户进行分类，就有先验的分类要求

7.机器学习常见评估指标  
模型预测效果评价，通常用相对绝对误差(AE,RE)、平均绝对误差（MAE）、根均方差(MSE)、均方根误差(RMSE)等指标来衡量  

8.请解释过拟合，以及如何防止过度拟合  
过拟合：是指为了得到一致假设而使假设变得过度严格判断过拟合的方法：一个假设（模型）在训练数据上能够获得比其他假设（模型）  
更好的拟合， 但是在【训练数据外】 的数据集上却不能很好地拟合数据，这就意味着出现了过拟合现象。  
解决方法有：增大数据量、交叉验证、正则化特征、减少特征、减少权值  

9.请尝试向非技术人员阐释交叉验证  
将数据样本切割成较小的子集，一部分用于训练模型，一部分用于验证模型（训练集的规模比验证集的规模大得多），利用验证集来测试训练得到的模型,主要用于  
评估模型的性能。通过模型在训练集上的表现和在验证集上的表现差异，来评估模型的泛化能力，和最终确定模型    
常见的有：k-folds 交叉验证，leave-one-out法  
k-folds: 将初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次  
平均K次的结果或者使用其它结合方式，最终得到一个单一估测  

10.监督学习和无监督学习有什么区别  
监督学习：对具有标记（分类）的训练样本进行学习，这里，所有的标记（分类）是已知的。 如：决策树算法、朴素贝叶斯算法、 KNN 算法等  
无监督学习：对没有标记（分类）的训练样本进行学习，目的是为了发现训练集中的结构特性。这里，所有的标记（分类）是未知的。 如：聚类算法  

11.常见分类、聚类、回归算法有区别以及适用场景


12.怎么处理数据中的离群值  
离群值的存在会影响到对数据的拟合和预测，通常需要加以处理，大致可以分为两类方法  
第一类的方法可以参考缺失值处理：1）直接删除 2）替换：可以使用均值、中位数、众数进行替换  
第二类的方法是离群值处理特有的：利用拉依达准则法（3σ准则），将超出这个范围的值替换成设定的阈值，通常为均值±3σ、均值±2σ，视情况而定  

13.用于评估预测模型的矩阵称为什么
混淆矩阵（confusion matrix），其列代表预测的类别，行代表真实值的分类

14.简述k-means聚类的基本思想、步骤以及k-means的缺点  
k-means的基本思想：  
通过迭代寻找k个聚类的一种划分方案，使得用这k个聚类的均值来代表相应各类样本时所得的总体误差最小。  
k-means算法的基础是最小误差平方和准则  

K-means 聚类步骤： 
Step1: 随机选择 k 个质心（即 k 个类）  
Step2: 计算每一个点到这些质心的距离，然后决定每个点所属的类    
Step3: 对于每个类，重新确定该类的质心  
Step4: 若收敛，则结束；否则转到 Step2  

K-means缺点：
1.对聚类中心的初始化比较敏感，不同的初始化带来不同的聚类结果  
2.K值需要首先人工确定(启发式)  
3.只能处理服从标准正太分布的聚类  
4.K-means对于噪声比较敏感  
 

15.bagging和boosting的区别  
Boosting和Bagging都是组合多个分类器投票的方法，二者均是根据单个分类器的正确率决定其权重。Bagging与Boosting的区别：取样方式不同  
Bagging采用均匀取样，而Boosting根据错误率取样。Bagging的各个预测函数没有权重，而Boosting是由权重的，Bagging的各个预测函数可以并行生成  
而Boosting的各个预测函数只能顺序生成  

16.LR和SVM的联系与区别  
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 
2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的  
区别： 
1、LR是参数模型，SVM是非参数模型  
2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重  
3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重  
4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算  
5、logic能做的svm能做，但可能在准确率上有问题，svm能做的logic有的做不了  

17.正则化  
正则化是针对过拟合而提出的，因为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。
L2正则化：目标函数中增加所有权重w参数的平方之和, 逼迫所有w尽可能趋向零但不为零. 因为过拟合的时候, 拟合函数需要顾忌每一个点, 最终形成的拟合函数波动很大, 在某些很小的区间里, 函数值的变化很剧烈, 也就是某些w非常大. 为此, L2正则化的加入就惩罚了权重变大的趋势  
L1正则化：目标函数中增加所有权重w参数的绝对值之和, 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0, 奔向零的速度不如L1给力了)  

18.LR与线性回归的区别与联系  
逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题

19.决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么  
Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器, 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器, 迭代集成(平滑加权)  
决策树属于最常用的学习器, 其学习过程是从根建立树, 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂, CART决策树用基尼指数计算最优分裂, xgboost决策树使用二阶泰勒展开系数计算最优分裂  

20.为什么xgboost要用泰勒展开，优势在哪里？  
xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归  

21.xgboost如何寻找最优特征？是又放回还是无放回的呢？  
xgboost在训练的过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据, 从而记忆了每个特征对在模型训练时的重要性 -- 从根到叶子中间节点涉及某特征的次数作为该特征重要性排序.  
xgboost属于boosting集成学习方法, 样本是不放回的, 因而每轮计算样本不重复. 另一方面, xgboost支持子采样, 也就是每轮计算可以不使用全部样本, 以减少过拟合. 进一步地, xgboost 还有列采样, 每轮计算按百分比随机采样一部分特征, 既提高计算速度又减少过拟合  

22.谈谈判别式模型和生成式模型  
判别方法：由数据直接学习决策函数Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型    
生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型  
由生成模型可以得到判别模型，但由判别模型得不到生成模型  
常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场  
常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）  

23.plsa和lda  
LDA是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布，只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。dirichlet先验为某篇文档随机抽取出某个主题分布和词分布。plsa认为主题分布和词分布是唯一确定的  

24.机器学习中，为何要经常对数据做归一化  
1）归一化后加快了梯度下降求最优解的速度  2）归一化有可能提高精度

25.哪些机器学习算法不需要做归一化处理
概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。之所以进行数据归一化是因为各维度的量纲不相同，同时作为变量的时候可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能量纲也需要调整  

26.请简要说说一个完整机器学习项目的流程  
1 抽象成数学问题  
2 获取数据  
3 特征预处理与特征选择  
4 训练模型与调优  
5 模型诊断  
6 模型融合  
7 上线运行  

27.特征离散化优点  
1.离散特征的增加和减少都很容易，易于模型的快速迭代  
2.稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展  
3.离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰  
4.逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合 
5.离散化后可以进行特征交叉，由M+N个变量变为MN个变量，进一步引入非线性，提升表达能力  
6.特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问  
7.特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险  
模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验   

28.简单说下sigmoid激活函数  
非线性激活函数，相当于把一个实数压缩至0到1之间

29.什么是卷积  
对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作  

30.数据不平衡问题  
1.采样，对小样本加噪声采样，对大样本进行下采样  
2.数据生成，利用已知样本生成新的样本  
3.进行特殊的加权，如在Adaboost中或者SVM中  
4.采用对不平衡数据集不敏感的算法  
5.改变评价标准：用AUC/ROC来进行评价  
6.采用Bagging/Boosting/ensemble等方法  
7.在设计模型的时候考虑数据的先验分布  
